enabled: False         # Whether to use UP.
dp: False              # Data parallelism
init_mode: default     # Weight initialization algorithm for the source model
init_mult_max: 5.0     # Weight multiplier
mask_prob_max: 0.7     # Weight masking probability
buffer_size: 2000      # Number of instances to keep in the buffer
temperature: 0.5       # Sampling temperature from model probabilities
reset_prob: 0.01       # Proportion of the buffer to reset each sample
source_mode: nn        # How to generate the up data: nn (use a neural network) or aut (a random NDA)
num_batches: 40_000    # Number of pre-training batches
spinup: 5_000          # Number of spinup batches, in which the model isn't trained, but we run the buffer through the
                       # source model
nonlinearity: relu     # NL to use in the nnsimple source model.
iterations: 1          # How many times to iterate the nnsimple source model.
batch_size: 40         # Model batch size
print_every: 1000      # How often to print from the buffer
source_layers: 8       # How many trf layers to use in the source model (this is half the nr of the target model)
sample_batch_size: 100 # Number of instances in the buffer to pass
                       # through the model each iteration
gc: 1.0                # Gradient clipping
mask_token: 4          # Which masking token to use (4 in the default tokenizer). We hard-code this because the
                       # tokenizer is loaded after the UP. (see backend/utils.py:170)
mlm_probability: 0.15  # Proportion of tokens to mask
accumulate: 1          # How many micro-batches to accumulate over for one update step.
acc_warmup: -1         # if > 0, the accumulation is linearly ramped up over this many instances.
warmup: 100_000        # Learning rate Warmup (in instances)
cooldown: 500_000      # If true, the learning rate is cooled down linearly to 0 after the peak lr is reached.
lr: 1e-4               # (peak) Learning rate
weight_decay: 0.0      # Weight decay
betas: [0.9, 0.999]    # Adam beta values (these are the torch defaults, cramming phase uses (0.9, 0.98)
reuse_opt: False       # Whether to reuse the optimizer used in up for the data-based pretraining
opt_mult: 1.0          # A multiplier for the up optimizer state, before it's applied to the fine-tuning
eval_ood_every: -1     # How often to run an evaluation on the OOD datasets during u-pretraining
eval_samples: 10_000   # How many samples to evaluate on for the OOD datasets
use_80_20_rule: True   # Replace 20% of the masked tokens with thre true tokens or a corrupted token
transfer: discrete     # How information is transfered from the source to the model. 'discrete' samples a sequence from
                       # the source. 'distill' uses distillation on the logits produced by the model. 'continuous' uses
                       # continuous input/output vectors to train the core of the model without the token embeddings and
                       # language head.
