enabled: False         # Whether to use UP.
dp: False              # Data parallelism
init_mode: default     # Weight initialization algorithm for the source model
init_mult_max: 5.0     # Weight multiplier
mask_prob_max: 0.7     # Weight masking probability
buffer_size: 2000      # Number of instances to keep in the buffer
temperature: 0.5       # Sampling temperature from model probabilities
reset_prob: 0.01       # Proportion of the buffer to reset each sample
source_mode: nn        # How to generate the up data: nn (use a neural network) or aut (a random NDA)
num_batches: 40_000    # Number of pre-training batches
spinup: 5_000          # Number of spinup batches, in which the model isn't trained, but we run the buffer through the
                       # source model
nonlinearity: relu     # NL to use in the nnsimple source model.
iterations: 1          # How many times to iterate the nnsimple source model.
batch_size: 40         # Model batch size
print_every: 1000      # How often to print from the buffer
source_layers: 8       # How many trf layers to use in the source model (this is half the nr of the target model)
bid_source: False      # Whether the source is causal (False) or bidirectional (True)
sample_batch_size: 100 # Number of instances in the buffer to pass
                       # through the model each iteration
gc: 1.0                # Gradient clipping threshold. We use _adaptive gradient clipping_. That is, we take a running
                       # mean and running standard deviation of the gradient norm, and if the current gradient norm is
                       # higher than the mean plus `gc` times the standard deviation, we clip to that value.
mask_token: 4          # Which masking token to use (4 in the default tokenizer). We hard-code this because the
                       # tokenizer is loaded after the UP. (see backend/utils.py:170)
mlm_probability: 0.15  # Proportion of tokens to mask
accumulate: 512        # Macrobatch size (after warmup)
acc_warmup: -1         # if > 0, the accumulation is linearly ramped up over this many instances.
acc_start: 0           # Instances seen at which to start the acc warmup
warmup: 100_000        # Learning rate Warmup (in instances)
cooldown: 10_000_000      # If true, the learning rate is cooled down linearly to 0 after the peak lr is reached.
lr: 1e-4               # (peak) Learning rate
weight_decay: 0.0      # Weight decay
betas: [0.9, 0.999]    # Adam beta values (these are the torch defaults, cramming phase uses (0.9, 0.98)
reuse_opt: False       # Whether to reuse the optimizer used in up for the data-based pretraining
opt_mult: 1.0          # A multiplier for the up optimizer state, before it's applied to the fine-tuning
eval_ood_every: -1     # How often to run an evaluation on the OOD datasets during u-pretraining
eval_samples: 10_000   # How many samples to evaluate on for the OOD datasets
use_80_20_rule: True   # Replace 20% of the masked tokens with thre true tokens or a corrupted token
transfer: discrete     # How information is transferred from the source to the model. 'discrete' samples a sequence from
                       # the source. 'distill' uses distillation on the logits produced by the model. 'continuous' (TODO)
                       # uses continuous input/output vectors to train the core of the model without the token embeddings
                       # and language head.
loss_mask_scale: 0.0   # In (UP) distill mode, how much to scale down the non-manipulated elements. In plain MLM training,
                       # these are masked out, because they're easy to predict. However, in distill mode, we _sample_ the
                       # input from the logit, so recovering the logit is actually a challenging task, so including them
                       # in the loss, potentially with a scaling factor, creates a richer signal to learn from.
                       # This value is a multiplier for the loss of the non-manipulated elements; 0.0 corresponds to
                       # masking them out as in the plain MLM loss.
snapshot_file: null    # Where to save the model/opt snapshot after pretraining is finished. If None, no snapshot is
                       # saved.
snapshot: null         # Which snapshot to load. If None, a u-pretraining run is performed to create a fresh snapshot.
up_mix: 0.0            # Probability that a dp instance is replaced with a up instance.
up_mix_decay: 0.0      # This is subtracted from the up_mix every batch to decay it linearly
nrehearsal: 10_000     # Number of UP batches to generate for the rehearsal phase
reset_betas: false     # Reset the betas after UP, to the value of `betas`
reset_wd: false        # Reset the weight decay parameter of the optimizer to the Cramming value
manual: false          # Do the DP training manually, bypassing the cramming engine.
mode: 'init'           # Model transfer mode: init (initialize weights from the DP model), norm (add an aux loss term
                       # for the distance between the model weight and the dp model weights), distill (distillation on
                       # the output probabilities), none (no transfer, for debugging).
aux_alpha: -1.0         # The multiplier for the auxiliary loss (both 'distill' and 'norm'). In 'init' mode this is used to mix
                       # between the default init and the UP init.
which_aux: all         # Which parts of the model to compute the norm aux loss over.
data_path: '.'         # Where to store the dataset cache
early_stop: -1.0       # If positive, then stop the dp finetuning if the EMA of the loss gets below this value.
alpha_warmup: -1       # If pos, how long to warm up the alpha (during DP). Between 0 and 1 (proportion of time budget).
alpha_cooldown: -1     # If neg, how long to cool down the alpha (during DP). Between 0 and 1 (proportion of time budget).
                       # If a pair, we cool down from the first point to the second, and stay at 0 after that.
use_log_alpha: false   # Schedule the alpha log-linearly
log_alpha_min: 1e-12   # The minimum value for the alpha
up_only: false         # If true, kills the run after the UP phase (used for HP tuning)
lstmemb: 64            # Embedding for the LSTM source
lstmlayers: 1          # Number of layers in the LSTM source
lstmreset: [20,20]     # Number of instances to reset (constant and random)
lstmmult: [0,1.1]      # Range from which to sample the LSTM multiplier
lstmembmult: 1.0       # Multiplier for the embeddings of the LSTM source
lstmlinmult: 1.0       # Multiplier for the linear layer (output head)
lstmseed: 8            # Seed size for the LSTM source
lstmtemp: 1e-4         # Sampling temperature for the LSTM source


