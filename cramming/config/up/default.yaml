enabled: False         # Whether to use UP.
dp: False              # Data parallelism
init_mult_max: 5.0     # Weight multiplier
mask_prob_max: 0.7     # Weight masking probability
buffer_size: 2000      # Number of instances to keep in the buffer
temperature: 0.5       # Sampling temperature from model probabilities
reset_prob: 0.01       # Proportion of the buffer to reset each sample
num_batches: 40_000    # Number of pre-training batches
batch_size: 40         # Model batch size
sample_batch_size: 100 # Number of instances in the buffer to pass
                       # through the model each iterations
gc: 1.0                # Gradient clipping
mask_token: 4          # Which masking token to use (4 in the default tokenizer). We hard-code this because the
                       # tokenizer is loaded after the UP. (see backend/utils.py:170)
mlm_probability: 0.15  # Proportion of tokens to mask
accumulate: 1          # How many micro-batches to accumulate over for one update step.
acc_warmup: -1         # if > 0, the accumulation is linearly ramped up over this many instances.
warmup: 100_000        # Learning rate Warmup (in instances)
cooldown: true.        # If true, the learning rate is cooled down linearly to 0 after the peak lr is reached.
lr: 1e-4               # (peak) Learning rate
weight_decay: 0.0      # Weight decay
betas: [0.9, 0.999]    # Adam beta values (these are the torch defaults, cramming phase uses (0.9, 0.98)
reuse_opt: False       # Whether to reuse the optimizer used in up for the data-based pretraining
opt_mult: 1.0          # A multiplier for the up optimizer state, before it's applied to the fine-tuning
